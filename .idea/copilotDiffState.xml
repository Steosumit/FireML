<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/BrowserExtension/manifest.json">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/BrowserExtension/manifest.json" />
              <option name="originalContent" value="{&#10;  &quot;manifest_version&quot;: 3,&#10;  &quot;name&quot;: &quot;FireML Firewall&quot;,&#10;  &quot;version&quot;: &quot;0.1.0&quot;,&#10;  &quot;description&quot;: &quot;Blocks suspicious URLs by consulting the FireML local predictor.&quot;,&#10;  &quot;permissions&quot;: [&#10;    &quot;storage&quot;,&#10;    &quot;tabs&quot;,&#10;    &quot;webNavigation&quot;&#10;  ],&#10;  &quot;host_permissions&quot;: [&#10;    &quot;&lt;all_urls&gt;&quot;&#10;  ],&#10;  &quot;background&quot;: {&#10;    &quot;service_worker&quot;: &quot;background/background.js&quot;,&#10;    &quot;type&quot;: &quot;module&quot;&#10;  },&#10;  &quot;web_accessible_resources&quot;: [&#10;    {&#10;      &quot;resources&quot;: [&quot;ui/index.html&quot;],&#10;      &quot;matches&quot;: [&quot;&lt;all_urls&gt;&quot;]&#10;    }&#10;  ],&#10;  &quot;action&quot;: {&#10;    &quot;default_title&quot;: &quot;FireML Firewall&quot;&#10;  }&#10;}&#10;&#10;" />
              <option name="updatedContent" value="{&#10;  &quot;manifest_version&quot;: 3,&#10;  &quot;name&quot;: &quot;FireML Firewall&quot;,&#10;  &quot;version&quot;: &quot;0.1.0&quot;,&#10;  &quot;description&quot;: &quot;Blocks suspicious URLs by consulting the FireML local predictor.&quot;,&#10;  &quot;permissions&quot;: [&#10;    &quot;storage&quot;,&#10;    &quot;tabs&quot;,&#10;    &quot;webNavigation&quot;&#10;  ],&#10;  &quot;host_permissions&quot;: [&#10;    &quot;&lt;all_urls&gt;&quot;&#10;  ],&#10;  &quot;background&quot;: {&#10;    &quot;service_worker&quot;: &quot;background/background.js&quot;,&#10;    &quot;type&quot;: &quot;module&quot;&#10;  },&#10;  &quot;web_accessible_resources&quot;: [&#10;    {&#10;      &quot;resources&quot;: [&quot;ui/index.html&quot;],&#10;      &quot;matches&quot;: [&quot;&lt;all_urls&gt;&quot;]&#10;    }&#10;  ],&#10;  &quot;action&quot;: {&#10;    &quot;default_title&quot;: &quot;FireML Firewall&quot;&#10;  }&#10;}&#10;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/LocalServer/main.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/LocalServer/main.py" />
              <option name="originalContent" value="from fastapi import FastAPI, HTTPException&#10;from utils.models import PredictRequestModel&#10;from ml.model_loader import load_model&#10;from ml.predictor import predict_url&#10;from utils.validator import validate_request, RequestValidationError&#10;&#10;app = FastAPI()&#10;&#10;# init_service() - Initialize the ML model&#10;def init_service():&#10;    &quot;&quot;&quot;Ensure the ML model is loaded and ready.&quot;&quot;&quot;&#10;    return load_model()&#10;&#10;&#10;# api_predict(request) - Endpoint to handle prediction requests&#10;@app.post(&quot;/predict&quot;)&#10;async def api_predict(request: PredictRequestModel):&#10;    try:&#10;        validated_request = validate_request(request)&#10;    except RequestValidationError as exc:&#10;        raise HTTPException(status_code=422, detail=str(exc)) from exc&#10;&#10;    model = init_service()&#10;    prediction = predict_url(str(validated_request.url), model=model)&#10;    return {&#10;        &quot;url&quot;: str(validated_request.url),&#10;        &quot;decision&quot;: prediction[&quot;decision&quot;],&#10;        &quot;score&quot;: prediction[&quot;score&quot;],&#10;        &quot;features&quot;: prediction[&quot;features&quot;],&#10;    }&#10;&#10;&#10;# log_event(url, decision, timestamp) - Endpoint to log events&#10;" />
              <option name="updatedContent" value="from fastapi import FastAPI, HTTPException&#10;from utils.models import PredictRequestModel&#10;from ml.model_loader import load_model&#10;from ml.predictor import predict_url&#10;from utils.validator import validate_request, RequestValidationError&#10;&#10;app = FastAPI()&#10;&#10;# init_service() - Initialize the ML model&#10;def init_service():&#10;    &quot;&quot;&quot;Ensure the ML model is loaded and ready.&quot;&quot;&quot;&#10;    return load_model()&#10;&#10;&#10;# api_predict(request) - Endpoint to handle prediction requests&#10;@app.post(&quot;/predict&quot;)&#10;async def api_predict(request: PredictRequestModel):&#10;    try:&#10;        validated_request = validate_request(request)&#10;    except RequestValidationError as exc:&#10;        raise HTTPException(status_code=422, detail=str(exc)) from exc&#10;&#10;    model = init_service()&#10;    prediction = predict_url(str(validated_request.url), model=model)&#10;    return {&#10;        &quot;url&quot;: str(validated_request.url),&#10;        &quot;decision&quot;: prediction[&quot;decision&quot;],&#10;        &quot;score&quot;: prediction[&quot;score&quot;],&#10;        &quot;features&quot;: prediction[&quot;features&quot;],&#10;    }&#10;&#10;&#10;# log_event(url, decision, timestamp) - Endpoint to log events" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/LocalServer/ml/model_loader.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/LocalServer/ml/model_loader.py" />
              <option name="originalContent" value="# Code to load model&#10;from pathlib import Path&#10;from threading import Lock&#10;from typing import Optional&#10;import joblib&#10;&#10;DEFAULT_MODEL_PATH = Path(__file__).resolve().parents[1] / &quot;model&quot; / &quot;url_classifier_RandomForest_CPU.pkl&quot;&#10;_model: Optional[object] = None&#10;_model_lock = Lock()  # for thread safe operation (read Notes for details)&#10;&#10;def load_model(path: Optional[Path] = None):&#10;    &quot;&quot;&quot;Load the ML model once and cache it for future predictions.&quot;&quot;&quot;&#10;    global _model&#10;&#10;    if _model is not None:&#10;        return _model&#10;&#10;    resolved_path = Path(path or DEFAULT_MODEL_PATH).expanduser().resolve()&#10;&#10;    with _model_lock:&#10;        if _model is None:&#10;            if not resolved_path.exists():&#10;                raise FileNotFoundError(f&quot;Model file not found at {resolved_path}&quot;)&#10;            _model = joblib.load(resolved_path)  # load the model if it is not laoded yet&#10;    return _model&#10;&#10;&#10;def reload_model(path: Optional[Path] = None):&#10;    &quot;&quot;&quot;Force reload the model, useful during development/testing.&quot;&quot;&quot;&#10;    global _model&#10;    _model = None&#10;    return load_model(path)&#10;" />
              <option name="updatedContent" value="# Code to load model&#10;from pathlib import Path&#10;from threading import Lock&#10;from typing import Optional&#10;import joblib&#10;&#10;DEFAULT_MODEL_PATH = Path(__file__).resolve().parents[1] / &quot;model&quot; / &quot;url_classifier_RandomForest_CPU.pkl&quot;&#10;_model: Optional[object] = None&#10;_model_lock = Lock()  # for thread safe operation (read Notes for details)&#10;&#10;def load_model(path: Optional[Path] = None):&#10;    &quot;&quot;&quot;Load the ML model once and cache it for future predictions.&quot;&quot;&quot;&#10;    global _model&#10;&#10;    if _model is not None:&#10;        return _model&#10;&#10;    resolved_path = Path(path or DEFAULT_MODEL_PATH).expanduser().resolve()&#10;&#10;    with _model_lock:&#10;        if _model is None:&#10;            if not resolved_path.exists():&#10;                raise FileNotFoundError(f&quot;Model file not found at {resolved_path}&quot;)&#10;            _model = joblib.load(resolved_path)  # load the model if it is not laoded yet&#10;    return _model&#10;&#10;&#10;def reload_model(path: Optional[Path] = None):&#10;    &quot;&quot;&quot;Force reload the model, useful during development/testing.&quot;&quot;&quot;&#10;    global _model&#10;    _model = None&#10;    return load_model(path)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>